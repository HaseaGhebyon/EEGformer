{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "from os.path import basename\n",
    "import os\n",
    "\n",
    "LINK_LIST = './_link_.txt'\n",
    "\n",
    "with open(\"test.txt\") as file:\n",
    "    urls = [line.rstrip() for line in file]\n",
    "    file.close()\n",
    "\n",
    "download_urls = []\n",
    "\n",
    "for url in urls:\n",
    "    # URL halaman yang ingin di-crawl\n",
    "\n",
    "    # Mendapatkan konten halaman\n",
    "    response = requests.get(url)\n",
    "    web_content = response.content\n",
    "\n",
    "    # Parsing konten halaman menggunakan BeautifulSoup\n",
    "    soup = BeautifulSoup(web_content, 'html.parser')\n",
    "\n",
    "    links =  soup.select(\"a[href]\")\n",
    "\n",
    "    # Menyimpan semua href dari link tersebut\n",
    "    hrefs = [link.get('href') for link in links]\n",
    "\n",
    "    # Menampilkan semua link yang ditemukan\n",
    "    for href in hrefs:\n",
    "        if \"ndownloader\" in href:\n",
    "            download_urls.append(href)\n",
    "            print(url)\n",
    "            print(href)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOADER"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
